{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMYFwPxMyPnIq5OvTuUIK6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vin136/Machine-Learning-Interview-Questions/blob/main/nbs/Deep_LEARNING_NOTES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ6dN_GyGuNB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization (refer cowan's notes)\n",
        "\n",
        "Gradient descent => Taylor series expansion : F(x) ≈ F(xt) + F′(xt)(x − xt) + 1/2F′′(xt)(x − xt)^2.\n",
        "\n",
        "`Direction` : This tells under second order approximation => move opposite to direction of grad => func value decreases. \n",
        "\n",
        "`Amount` : How much to move => roughly on the order of ``inverse second derivative``. More curvature= move slowly.\n",
        "\n",
        "\n",
        "Momentum: At any point continue moving a small part in the direction previously moved.\n",
        "\n",
        "$$x_{t+1} = x_t − α_t∇F (x_t) + β_t(x_{t} − x_{t−1})$$\n",
        "\n",
        "`Intuition`: Using the information of both current and previous gradient = approximating second order methods/ proxy estimation of curvature.\n",
        "\n",
        "What's Good Step-Size ?\n",
        "\n",
        "`ADAGRAD` : If the past updates for a component has been smaller => less curvature => can use higher learning rate.\n",
        "Method: [Refer](https://github.com/vin136/Machine-Learning-Interview-Questions/blob/main/nbs/Screen%20Shot%202023-04-17%20at%205.57.35%20PM.png)\n",
        "\n",
        "Flaw:  because the accumulated gradients Git are perpetually increasing by positive amounts, the stepsizes will be decreasing to zero. If this happens to quickly, then the algorithm effectively ‘freezes’ and is unable to improve.\n",
        "\n",
        "`RMS PROP`: Instead of taking sum of past gradients along a direction, we can take decaying average [METHOD](https://github.com/vin136/Machine-Learning-Interview-Questions/blob/main/nbs/Screen%20Shot%202023-04-17%20at%206.01.27%20PM.png)\n",
        "\n",
        "`ADAM`: [RMSPROP+ MOMENTUM](https://github.com/vin136/Machine-Learning-Interview-Questions/blob/main/nbs/Screen%20Shot%202023-04-17%20at%206.11.31%20PM.png)"
      ],
      "metadata": {
        "id": "yDB7zqOJKlIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architectures\n",
        "\n",
        "\n",
        "Efficiency, modularity and reuse.\n",
        "\n",
        "1. Residual Connections: use a residual block.\n",
        "\n",
        "eg: resnet\n",
        "\n",
        "```\n",
        "“x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "residual = x                                                    ❶\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)  ❷\n",
        "x = layers.MaxPooling2D(2, padding=\"same\")(x)                   ❷\n",
        "residual = layers.Conv2D(64, 1, strides=2)(residual)            ❸\n",
        "x = layers.add([x, residual])”\n",
        "```\n",
        "\n",
        "You can use 1*1 conv or padding to match the sizes.\n",
        "\n",
        "Further Improvement: [DenseNet](https://github.com/vin136/Machine-Learning-Interview-Questions/blob/main/nbs/Screen%20Shot%202023-04-17%20at%206.29.30%20PM.png)=> Concatenate Instead of add\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. Conv2D = 1*1 conv + depthwise seperable conv\n",
        "\n",
        "eg: xception net.\n",
        "\n",
        "intuition: “ separating the learning of spatial features and the learning of channel-wise features”\n",
        "\n"
      ],
      "metadata": {
        "id": "9cKww95aSD-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers"
      ],
      "metadata": {
        "id": "UBTqoDVKWDBo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQchQEQOWJOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_tFjDY4aL7kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Data\n",
        "\n",
        "1. [Best course for cv](https://www.youtube.com/watch?v=igP03FXZqgo&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&index=20)"
      ],
      "metadata": {
        "id": "l_RRZIukkgp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Important Papers:\n",
        "\n",
        "1."
      ],
      "metadata": {
        "id": "1Ecro6V8kkHQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}